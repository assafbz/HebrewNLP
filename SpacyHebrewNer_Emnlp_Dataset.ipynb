{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpacyHebrewNer-Emnlp Dataset.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPCDrrNhuLFVEiRsoWsXL2l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/assafbz/HebrewNLP/blob/master/SpacyHebrewNer_Emnlp_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNHltCvEtVEU",
        "colab_type": "text"
      },
      "source": [
        "# Download EMNLP Dataset\n",
        "Downloaded from https://sites.google.com/site/rmyeid/projects/polylgot-ner \n",
        "Al-Rfou, Rami for Polyglot Work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGjZpVUqkb80",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "5e4b8292-1659-4b32-b284-e8cd64cdbb8c"
      },
      "source": [
        "!pip install wget\n",
        "import wget\n",
        "url = 'http://cs.stonybrook.edu/~polyglot/ner2/emnlp_datasets.tgz'\n",
        "wget.download(url)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=ad1e9fc79624275ffc0bdf7dd4a2158027a8ad977dac7aa20cfdfdff49eaea47\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'emnlp_datasets.tgz'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ1AX2Nktsvl",
        "colab_type": "text"
      },
      "source": [
        "# Extract Data\n",
        "Only Hebrew Folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwtcd442laWT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2bf5908d-683d-4d39-82b8-e46df8b200e7"
      },
      "source": [
        "!tar -xvf emnlp_datasets.tgz acl_datasets/he"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acl_datasets/he/\n",
            "acl_datasets/he/data/\n",
            "acl_datasets/he/data/he_wiki.conll\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhoyRl1FtynQ",
        "colab_type": "text"
      },
      "source": [
        "# Prepare train data for SpaCy\n",
        "note that text are rebuilt from tagged corpus cause no original texts were supplied"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4qQycB-m-Cd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c3e774f-e1ce-4c76-eff7-a388d8cc6a36"
      },
      "source": [
        "train_data = []\n",
        "entities = []\n",
        "sentence = \"\"\n",
        "offset = 0\n",
        "with open('acl_datasets/he/data/he_wiki.conll', 'r') as f:\n",
        "    line = f.readline()\n",
        "    while line:\n",
        "      if line != '\\n':\n",
        "        token, tag = line.split('\\t')\n",
        "        tag = tag.rstrip()\n",
        "        if tag != 'O':\n",
        "          entity = (offset, offset + len(token), tag)\n",
        "          entities.append(entity)\n",
        "        sentence = sentence + token + ' '\n",
        "        offset += len(token) + 1\n",
        "\n",
        "      else:\n",
        "        train_data.append((sentence, {\"entities\": entities}))\n",
        "        entities = []\n",
        "        sentence = \"\"\n",
        "        offset = 0\n",
        "\n",
        "      line = f.readline()\n",
        "      \n",
        "print(\"Retrieved\", len(train_data), \"Samples\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Retrieved 459933 Samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d20cnd0kt_FU",
        "colab_type": "text"
      },
      "source": [
        "# Check train data is correct and offsets are aligned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tRpnu9Gqgzb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4a925bd4-0f6b-4d30-aef5-9e1215bd6d29"
      },
      "source": [
        "sample = train_data[2]\n",
        "for entity in sample[1][\"entities\"]:\n",
        "    print(sample[0][entity[0]:entity[1]], entity[2])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "וורשה LOC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46_YasgYuFQb",
        "colab_type": "text"
      },
      "source": [
        "# Get Labels\n",
        "Keep only distinct labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiiVNLFBqvaY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47fe64a4-5903-4ed5-e400-c454ccde6f30"
      },
      "source": [
        "all_labels = []\n",
        "for doc in train_data:\n",
        "    for entity in doc[1]['entities']:\n",
        "        all_labels.append(entity[2])\n",
        "labels = list(set(all_labels))\n",
        "labels"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ORG', 'PER', 'LOC']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfjns_pZuJjK",
        "colab_type": "text"
      },
      "source": [
        "# Build Spacy Hebrew Ner Model\n",
        "basic training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vlWbvBtrOYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "import random\n",
        "spacy.require_gpu()\n",
        "nlp = spacy.blank(\"he\")\n",
        "ner = nlp.create_pipe(\"ner\")\n",
        "nlp.add_pipe(ner, last=True)\n",
        "\n",
        "random.shuffle(train_data)\n",
        "for label in labels:\n",
        "    ner.add_label(label)\n",
        "optimizer = nlp.begin_training()\n",
        "for text, annotations in train_data:\n",
        "    nlp.update([text], [annotations])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G12ds_zUuOTD",
        "colab_type": "text"
      },
      "source": [
        "# Test On Train Data Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSZc8-3YrnB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_text = train_data[1][0]\n",
        "print(test_text)\n",
        "print('==========================')\n",
        "doc = nlp(test_text)\n",
        "for ent in doc.ents:\n",
        "    print(ent.label_, ent.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy5J8fo8uR20",
        "colab_type": "text"
      },
      "source": [
        "# Test On New Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYFxSuYXrsoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_text = \"כבר שנים רבות שמשרד הבריאות אינו מאפשר לאזרחי ישראל לבצע פעילות בסיסית\"\n",
        "print(test_text)\n",
        "print('==========================')\n",
        "doc = nlp(test_text)\n",
        "for ent in doc.ents:\n",
        "    print(ent.label_, ent.text)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}